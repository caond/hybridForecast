library(dplyr)
library(zoo)
library(xgboost)
library(ranger)
library(nnet)
library(lightgbm)
library(tseries)
library(ggplot2)
library(changepoint)
library(lubridate)

VERY_LARGE<-1E34
EPSILON<-1/VERY_LARGE

smape <- function(a, f) {
  a[which(a==Inf)]<- VERY_LARGE
  (1/length(a) * sum(2*abs(f-a) / (abs(a+EPSILON)+abs(f+EPSILON))))
}

auto_seasonal_periods <- function(ds, min_obs = 3) {
  # ds: vector of Date or POSIXct timestamps

  if (length(ds) < min_obs) stop("Not enough data points to detect frequency")

  ds <- sort(ds)
  diff_days <- as.numeric(median(diff(as.Date(ds)), na.rm = TRUE))

  # Define frequency thresholds (in days)
  freq <- case_when(
    diff_days < 1 ~ "hourly",
    diff_days < 2 ~ "daily",
    diff_days < 8 ~ "weekly",
    diff_days < 31 ~ "monthly",
    diff_days < 92 ~ "quarterly",
    TRUE ~ "yearly"
  )

  message("Detected frequency: ", freq)

  # Map to seasonal periods
  seasonal_periods <- switch(freq,
                             "hourly" = list(
                               daily = 24,
                               weekly = 24 * 7,
                               yearly = 24 * 365.25
                             ),
                             "daily" = list(
                               weekly = 7,
                               yearly = 365.25
                             ),
                             "weekly" = list(
                               yearly = 52
                             ),
                             "monthly" = list(
                               yearly = 12,
                               quarterly = 3
                             ),
                             "quarterly" = list(
                               yearly = 4
                             ),
                             "yearly" = list()  # Usually no seasonality
  )

  return(seasonal_periods)
}



create_fourier_terms <- function(t, period, prefix, K) {


  terms <- list()
  for (k in 1:K) {
    terms[[paste0(prefix, "_sin", k)]] <- sin(2 * pi * k * t / period)
    terms[[paste0(prefix, "_cos", k)]] <- cos(2 * pi * k * t / period)
  }

  return(as.data.frame(terms))

}



select_changepoints <- function(y, n_changepoints = NA, method = NA) {
  n <- length(y)

  if (!is.numeric(y)) stop("Input 'y' must be numeric.")

  if (is.na(n_changepoints)) {
    if (is.na(method)) {
      n_changepoints <- min(25, floor(n / 25))
      t_range <- 1:floor(0.9 * n)
      cps <- quantile(t_range, probs = seq(0.05, 0.95, length.out = n_changepoints))
    } else {
      # You could allow method selection here
      cpt <- changepoint::cpt.meanvar(y, method = method)
      cps <- changepoint::cpts(cpt)
    }
  } else {
    t_range <- 1:floor(0.9 * n)
    cps <- quantile(t_range, probs = seq(0.05, 0.95, length.out = n_changepoints))
  }

  return(as.numeric(cps))
}



create_trend_features <- function(n, n_changepoints) {
  t <- 1:n
  A <- sapply(n_changepoints, function(sj) pmax(0, t - sj))
  X <- cbind(t, A)
  colnames(X) <- c("t", paste0("cp_", n_changepoints))
  return(X)
}

auto_pick_K_all_components <- function(t, seasonal_periods, max_fraction = 0.05, max_K_cap = 5, min_K = 1) {
  n <- length(t)
  max_total_terms <- floor(n * max_fraction)

  sapply(seasonal_periods, function(period) {
    n_cycles <- n / period
    K <- floor(min(max_total_terms, n_cycles * 2))  # 2 terms per K (sin/cos)
    K <- min(K, max_K_cap)
    K <- max(K, min_K)
    return(K)
  })
}


fit_trend_seasonal_model <- function(y,seasonal_periods,n_changepoints,method) {
  n <- length(y)
  t <- 1:n

  # Trend
  changepoints <- select_changepoints(y, n_changepoints,method)
  trend_X <- create_trend_features(n, changepoints)



  # Seasonality

  # select K
  K_list<-auto_pick_K_all_components(t, seasonal_periods)

  seasonal_X_list <- lapply(names(seasonal_periods), function(name) {
    create_fourier_terms(t, seasonal_periods[[name]], prefix = name, K=K_list[[name]])
  })
  seasonal_X <- do.call(cbind, seasonal_X_list)

  # Combine
  X <- cbind(trend_X, seasonal_X)
  model <- lm(y ~ . -1, data = data.frame(y = y, X))
  betas <- coef(model)
  betas <- betas[!is.na(betas)]
  trend_vals <- as.vector(as.matrix(trend_X) %*% betas[colnames(trend_X)])

  seasonality_outputs <- list()
  for (i in seq_along(seasonal_X_list)) {
    mat <- as.matrix(seasonal_X_list[[i]])
    mat_cols <- colnames(mat)

    # Keep only columns that have corresponding non-NA coefficients
    valid_cols <- intersect(mat_cols, names(betas))
    mat_valid <- mat[, valid_cols, drop = FALSE]
    beta_valid <- betas[valid_cols]

    # Compute seasonal component for this group
    seasonality_outputs[[i]] <- as.vector(mat_valid %*% beta_valid)
  }

  names(seasonality_outputs)<-names(seasonal_periods)
  return(list(
    model = model,
    changepoints = changepoints,
    seasonal_periods = seasonal_periods,
    n_train = n,
    K_list=K_list, #fourier components
    coef_names = colnames(X),
    trend=trend_vals,
    seasonality=seasonality_outputs

  ))
}

predict_trend_seasonal_components <- function(model_obj, h) {
  n_future <- model_obj$n_train + h
  t_pred <- (model_obj$n_train + 1):n_future

  # Trend
  trend_X <- sapply(model_obj$changepoints, function(sj) pmax(0, t_pred - sj))
  trend_X <- cbind(t = t_pred, trend_X)
  colnames(trend_X) <- c("t", paste0("cp_", model_obj$changepoints))

  # Seasonality
  K_list<-model_obj$K_list
  season_components <- list()
  for (name in names(model_obj$seasonal_periods)) {
    season_components[[name]] <- create_fourier_terms(
      t_pred, model_obj$seasonal_periods[[name]], prefix = name,K_list[[name]]
    )
  }
  seasonal_X <- do.call(cbind, unname(season_components))

  # Full matrix for prediction
  full_X <- data.frame(cbind(trend_X, seasonal_X))
  preds <- predict(model_obj$model, newdata = full_X)

  # Compute each component separately
  betas <- coef(model_obj$model)
  betas <- betas[!is.na(betas)]
  trend_cols <- colnames(trend_X)
  component_trend <- as.matrix(trend_X) %*% betas[trend_cols]

  component_season <- list()
  for (name in names(model_obj$seasonal_periods)) {
    # Get seasonal columns with valid coefficients
    season_cols <- grep(paste0("^", name, "_"), names(betas), value = TRUE)

    # Only keep valid seasonal cols that exist in both matrix and beta
    season_cols <- intersect(season_cols, colnames(season_components[[name]]))
    if (length(season_cols) > 0) {
      season_matrix <- as.matrix(season_components[[name]][, season_cols, drop = FALSE])
      component_season[[name]] <- season_matrix %*% betas[season_cols]
    } else {
      component_season[[name]] <- rep(0, nrow(season_components[[name]]))
    }
  }
  return(list(
    forecast = preds,
    trend = as.vector(component_trend),
    seasonality = component_season,
    t = t_pred
  ))
}



find_lags<-function(df,max_lag=20)
{
  adf_test<-adf.test(df$y)
  if (adf_test$p.value>0.05)  y <- diff(df$y)
  else y<- df$y
  pacf_vals <- pacf(y, lag.max = max_lag, plot = FALSE)
  threshold <- 2 / sqrt(length(y))
  significant_lags <- which(abs(pacf_vals$acf) > threshold)
  significant_lags
}

prepare_trend_seasonal_component_prophet<-function(df,significant_lags)
{
  m <- prophet(df, daily.seasonality = TRUE,yearly.seasonality = TRUE,weekly.seasonality = TRUE)
  forecast_train <- predict(m, df)
  # Add trend and seasonality back to the original df
  df_feat <- df %>%
    mutate(
      trend = forecast_train$trend,
      daily= forecast_train$daily,
      weekly = forecast_train$weekly,
      yearly = forecast_train$yearly

    )
  for (i in significant_lags) {
    df_feat[[paste0("lag", i)]] <- dplyr::lag(df$y, i)
  }
  df_feat <- na.omit(df_feat)
  return(list(df_feat=df_feat,prophet_m=m))
}

prepare_trend_seasonal_component<-function(train,significant_lags,seasonal_periods,n_changepoints,method)
{

  ts_model <- fit_trend_seasonal_model(y=train$y,seasonal_periods=seasonal_periods, n_changepoints=n_changepoints,method=method)

  train_feat <- cbind(train, trend = ts_model$trend)

  # Add each seasonal component
  for (name in names(ts_model$seasonality)) {
    train_feat[[name]] <- ts_model$seasonality[[name]]
  }

  for (i in significant_lags) {
    train_feat[[paste0("lag", i)]] <- dplyr::lag(train$y, i)
  }
  train_feat <- na.omit(train_feat)
  return(list(train_feat=train_feat,ts_model=ts_model))
}



predict_one_step <- function(hybrid_model,newdata_feat) {

  model_used<-hybrid_model$model_used

  n_test <- nrow(newdata_feat)
  preds <- list()
  use_lags <- length(hybrid_model$significant_lags) > 0

  for (model in model_used)
  {

    preds[[model]]<-numeric(n_test)

    if (use_lags) {
      max_lags <- max(hybrid_model$significant_lags)
      lag_names <- paste0("lag", hybrid_model$significant_lags)
      recent_y <- tail(hybrid_model$train_feat$y, max_lags)
    }

    for (i in 1:n_test) {

      # Prepare lag features
      if (use_lags) {
          lag_values <- sapply(hybrid_model$significant_lags, function(lag) {
            recent_y[length(recent_y) - lag + 1]
          })
          lag_data<- as.data.frame(as.list(lag_values))
          names(lag_data) <- lag_names
          input_row <- cbind(newdata_feat[i, c("trend", names(hybrid_model$seasonal_periods))], lag_data)
      } else {
        input_row <- newdata_feat[i, c("trend", names(hybrid_model$seasonal_periods))]
      }
      newdata<- as.matrix(input_row)



      preds[[model]][i] <- predict(hybrid_model[[model]], newdata)

      # Update recent_y for next iteration
      if (use_lags) {
          recent_y <- tail(c(recent_y, preds[[model]][i]), max_lags)
      }
    }
  }

  return(preds)
}


hybrid_core<-function(train,model_used,seasonal_periods,n_changepoints,method,max_lag)
{
  significant_lags<-find_lags(train,max_lag)
  #prepare features
  feat_model<-prepare_trend_seasonal_component(train=train,significant_lags=significant_lags,seasonal_periods=seasonal_periods,n_changepoints=n_changepoints,method=method)
  #feat_model_prophet<-prepare_trend_seasonal_component_prophet(train,significant_lags)

  train_feat<-feat_model$train_feat
  features <- setdiff(colnames(train_feat),c("ds","y"))
  X_train<-train_feat[, features]
  y_train <- train_feat$y
  ## train xgboost model on train
  dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)

  dtrain_lgb <- lgb.Dataset(data = as.matrix(X_train), label = y_train)

  set.seed(123)

  xgb_model <- xgb.train(
    params = list(
      objective = "reg:squarederror",
      max_depth = 6,
      eta = 0.05,
      subsample = 0.8,
      colsample_bytree = 0.8,
      min_child_weight = 5
    ),
    data = dtrain,
    nrounds = 100,
    verbose = 0
  )
  formula_str <- paste("y ~", paste(features, collapse = " + "))
  nn_model <- nnet(
    formula =as.formula(formula_str),
    data=train_feat,
    size= max(10, floor(sqrt(NROW(train_feat) / (length(features) + 1)))),
    decay = 0.1,
    MaxNWts = 5000,
    linout = TRUE
  )


  #lm_model<-lm(as.formula(formula_str),data=train_feat)
  lgb_model<-lgb.train(
    params =
      list(
        objective = "regression",
        metric = "l2",
        learning_rate = 0.05,
        num_leaves = 31,
        max_depth=-1,
        min_data_in_leaf = 20,
        feature_fraction = 0.9,
        bagging_fraction = 0.8,
        bagging_freq = 1
      ),
    data = dtrain_lgb,
    nrounds = 100,
    verbose = -1
  )

  return(list(model_used=model_used,xgb=xgb_model,lgb=lgb_model,nn=nn_model,train_feat=train_feat,significant_lags=significant_lags,seasonal_periods=seasonal_periods,ts_model=feat_model$ts_model))
}




predict_hybrid_1<-function(hybrid_model,newdata)
{

  # prepare test data with prophet predict on trend and season
  res <- predict_trend_seasonal_components(hybrid_model$ts_model, h = NROW(newdata))
  newdata_feat <- cbind(newdata, trend = res$trend)

  # Add each seasonal component
  for (name in names(res$seasonality)) {
    newdata_feat[[name]] <- res$seasonality[[name]]
  }



  # then step by step predict the test

  yhat<-predict_one_step(hybrid_model,newdata_feat)

  # Initialize containers
  preds <-list()
  train <- list()
  residuals <- list()
  sigma <- list()
  yhat_lower <- list()
  yhat_upper <- list()

  # Forecast horizon and confidence interval setup
  h <- NROW(newdata_feat)
  ci_scale <- sqrt(1:h)
  z <- qnorm(0.975)  # 95% CI

  # Models to process

  # Generate predictions and compute residuals
  for (model in hybrid_model$model_used) {

    #  train[[model]] <- predict(hybrid_model[[model]], xgb.DMatrix(as.matrix(hybrid_model$train_feat[, hybrid_model$xgb_model$feature_names])))

    train[[model]] <- predict(hybrid_model[[model]], as.matrix(hybrid_model$train_feat[,!names(hybrid_model$train_feat) %in% c('ds','y')]))

    residuals[[model]] <- hybrid_model$train_feat$y - train[[model]]
    sigma[[model]] <- sd(residuals[[model]], na.rm = TRUE)
    # Compute confidence intervals for xgb predictions
    yhat_lower[[model]] <- yhat[[model]] - z * sigma[[model]] * ci_scale
    yhat_upper[[model]] <- yhat[[model]] + z * sigma[[model]] * ci_scale
    preds[[model]]$yhat<-yhat[[model]]
    preds[[model]]$yhat_lower<-yhat_lower[[model]]
    preds[[model]]$yhat_upper<-yhat_upper[[model]]
  }
  return(preds)

}


predict_hybrid<-function(hybrid_model,newdata)
{
  test_result<-predict_hybrid_1(hybrid_model,newdata)
  test_matrix<-do.call(rbind,test_result)
  yhat<-do.call(rbind,test_matrix[,'yhat'])
  yhat_lower<-do.call(rbind,test_matrix[,'yhat_lower'])
  yhat_upper<-do.call(rbind,test_matrix[,'yhat_upper'])

  yhat_ensemble<-hybrid_model$ensemble_weight %*% yhat
  yhat_ensemble_lower<-hybrid_model$ensemble_weight %*% yhat_lower
  yhat_ensemble_upper<-hybrid_model$ensemble_weight %*% yhat_upper

  hybrid_forecast<-list()
  hybrid_forecast$data<-hybrid_model$data
  hybrid_forecast$emsemble<-hybrid_model$emsemble
  hybrid_forecast$mape<-hybrid_model$mape
  newdata$yhat<-t(yhat_ensemble)
  newdata$yhat_lower<-t(yhat_ensemble_lower)
  newdata$yhat_upper<-t(yhat_ensemble_upper)
  hybrid_forecast$forecast<-newdata
  class(hybrid_forecast)<-'hybrid_forecast'
  return(hybrid_forecast)
}



hybrid <- function(data, model_used = c("xgb", "nn", "lgb"),
                   seasonal_periods = NULL,
                   n_changepoints = NA,
                   method = NA,
                   max_lag = NA,
                   horizon = 30,
                   max_fold = 5) {

  N <- nrow(data)

  if (is.null(seasonal_periods)) {
    seasonal_periods <- auto_seasonal_periods(data$ds)
  }

  if (is.na(max_lag)) {
    lag_data_length <- floor(log(N) * 10)
    seasonal_vals <- unlist(seasonal_periods)
    seasonal_vals <- seasonal_vals[seasonal_vals < N]
    seasonal_lags <- if (length(seasonal_vals) > 0) max(seasonal_vals) else 0
    max_lag <- min(max(lag_data_length, seasonal_lags), floor(N / 2))
  }

  model_choice <- rep(list(0:1), length(model_used))
  names(model_choice) <- model_used
  combinations <- expand.grid(model_choice) %>% tail(-1) %>% as.matrix()

  folds <- vector("list", max_fold)

  for (fold in seq_len(max_fold)) {
    cat("\nfold", fold)

    train_end <- N - horizon - fold + 1
    test_range <- (train_end + 1):(train_end + horizon)
    train_data <- data[1:train_end, ]
    test_data <- data[test_range, ]

    hybrid_model <- hybrid_core(train_data, model_used, seasonal_periods,
                                n_changepoints, method, max_lag)
    test_result <- predict_hybrid_1(hybrid_model, test_data)

    y <- test_data$y
    ds <- test_data$ds

    for (model in model_used) {
      test_result[[model]]$mape <- smape(y, test_result[[model]]$yhat)
    }

    # Combine forecasts
    test_matrix <- do.call(rbind, test_result)
    mape <- do.call(rbind, test_matrix[,"mape"])
    yhat <- do.call(rbind, test_matrix[,"yhat"])
    yhat_lower <- do.call(rbind, test_matrix[,"yhat_lower"])
    yhat_upper <- do.call(rbind, test_matrix[,"yhat_upper"])

    weights_matrix <- combinations
    for (i in seq_len(nrow(weights_matrix))) {
      idx <- which(weights_matrix[i, ] > 0)
      inv_mape <- 1 / mape[idx]
      inv_mape[!is.finite(inv_mape)] <- 1
      weights_matrix[i, idx] <- inv_mape
      weights_matrix[i, ] <- weights_matrix[i, ] / sum(weights_matrix[i, ])
      rownames(weights_matrix)[i] <- paste(model_used[idx], collapse = "|")
    }

    yhat_ensemble <- weights_matrix %*% yhat
    yhat_ensemble_lower <- weights_matrix %*% yhat_lower
    yhat_ensemble_upper <- weights_matrix %*% yhat_upper

    mape_by_method<-apply(yhat_ensemble,1,function(fcst){ smape(fcst,y)})

    folds[[fold]] <- list(
      y = y, ds = ds,
      yhat_ensemble = yhat_ensemble,
      yhat_ensemble_lower = yhat_ensemble_lower,
      yhat_ensemble_upper = yhat_ensemble_upper,
      mape_by_method = mape_by_method,
      combinations = weights_matrix
    )
  }

  # Combine folds
  mape_matrix <- do.call(cbind, lapply(folds, `[[`, "mape_by_method"))
  mean_mape <- rowMeans(mape_matrix)
  best_idx <- which.min(mean_mape)
  best_method <- names(mean_mape)[best_idx]
  best_mape <- mean_mape[best_idx]

  all_combinations <- do.call(rbind, lapply(folds, `[[`, "combinations"))
  best_weights <- colMeans(all_combinations[rownames(all_combinations) == best_method, , drop = FALSE])

  yhat_all <- do.call(cbind, lapply(folds, `[[`, "yhat_ensemble"))
  yhat_all_lower <- do.call(cbind, lapply(folds, `[[`, "yhat_ensemble_lower"))
  yhat_all_upper <- do.call(cbind, lapply(folds, `[[`, "yhat_ensemble_upper"))

  ds_all <- do.call(c,lapply(folds, `[[`, "ds"))
  y_all <- unlist(lapply(folds, `[[`, "y"))
  mape_all <- rep(mape_matrix[best_method, ], each = horizon)

  cv_data <- data.frame(
    fold = rep(1:max_fold, each = horizon),
    ds = ds_all,
    y = y_all,
    yhat = yhat_all[rownames(yhat_all) == best_method, ],
    yhat_lower = yhat_all_lower[rownames(yhat_all_lower) == best_method, ],
    yhat_upper = yhat_all_upper[rownames(yhat_all_upper) == best_method, ],
    mape = mape_all
  ) %>%
    group_by(fold) %>%
    mutate(fold_label = paste0("Fold ", fold, " (MAPE = ", round(mape[1] * 100, 1), "%)"))

  # Refit on full data
  final_model <- hybrid_core(data, model_used, seasonal_periods,
                             n_changepoints, method, max_lag)
  final_model$data<-data
  final_model$cv_data <- cv_data
  final_model$mape <- best_mape
  final_model$ensemble_weight <- best_weights
  final_model$emsemble <- best_method
  class(final_model)<-'hybrid_train'
  return(final_model)
}


plot.hybrid_train <- function(hybrid_model, x_lab = NULL, y_lab = NULL, ...) {

    cv_data <- hybrid_model$cv_data
    ds <- sort(unique(cv_data$ds))

    # Detect frequency using median diff in days
    diff_days <- as.numeric(median(diff(as.Date(ds)), na.rm = TRUE))
    freq <- case_when(
      diff_days < 1 ~ "hourly",
      diff_days < 2 ~ "daily",
      diff_days < 8 ~ "weekly",
      diff_days < 31 ~ "monthly",
      diff_days < 92 ~ "quarterly",
      TRUE ~ "yearly"
    )

    # Decide scale and format
    if (freq == "hourly") {
      cv_data$ds <- as.POSIXct(cv_data$ds)
      scale_func <- scale_x_datetime
      xbreaks <- list(date_labels = "%H:%M", date_breaks = "1 hour")
      if (is.null(x_lab)) x_lab <- "Hour"
    } else if (freq %in% c("daily", "weekly")) {
      scale_func <- scale_x_date
      xbreaks <- list(date_labels = "%m-%d", date_breaks = "1 day")
      if (is.null(x_lab)) x_lab <- "Date (mm-dd)"
    } else if (freq %in% c("monthly", "quarterly")) {
      scale_func <- scale_x_date
      xbreaks <- list(date_labels = "%m-%d", date_breaks = "1 month")
      if (is.null(x_lab)) x_lab <- "Date (mm-dd)"
    } else {  # yearly
      scale_func <- scale_x_date
      xbreaks <- list(date_labels = "%Y", date_breaks = "1 year")
      if (is.null(x_lab)) x_lab <- "Year"
    }

    if (is.null(y_lab)) y_lab <- "Value"

    if (!all(c("yhat_lower", "yhat_upper") %in% names(cv_data))) {
      stop("data must contain 'yhat_lower' and 'yhat_upper'.")
    }

    # Build plot
    p <- ggplot(cv_data, aes(x = ds)) +
      geom_ribbon(aes(ymin = yhat_lower, ymax = yhat_upper, group = fold_label),
                  fill = "lightblue", alpha = 0.4) +
      geom_line(aes(y = y, group = fold_label), color = "black", size = 1) +
      geom_line(aes(y = yhat, group = fold_label), color = "blue", linetype = "dashed", size = 1) +
      facet_wrap(~fold_label, scales = "free_x", ncol = 2) +
      do.call(scale_func, xbreaks) +
      labs(
        title = paste0("Actual vs Predicted (Ensemble = ", hybrid_model$emsemble,
                       ", MAPE = ", round(hybrid_model$mape * 100, 1), "%)"),
        x = x_lab,
        y = y_lab,
        ...
      ) +
      theme_minimal() +
      theme(
        legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1)
      )

    return(p)
}



plot.hybrid_forecast <- function(hybrid_forecast, x_lab = NULL, y_lab = NULL, ...) {

  data_combined <-bind_rows( tail(hybrid_forecast$data,min(NROW(hybrid_forecast$forecast),NROW(hybrid_forecast$data))),   hybrid_forecast$forecast)

  ds <- sort(unique(data_combined$ds))

  # Detect frequency using median diff in days
  diff_days <- as.numeric(median(diff(as.Date(ds)), na.rm = TRUE))
  freq <- case_when(
    diff_days < 1 ~ "hourly",
    diff_days < 2 ~ "daily",
    diff_days < 8 ~ "weekly",
    diff_days < 31 ~ "monthly",
    diff_days < 92 ~ "quarterly",
    TRUE ~ "yearly"
  )

  # Decide scale and format
  if (freq == "hourly") {
    data_combined$ds <- as.POSIXct(data_combined$ds)
    scale_func <- scale_x_datetime
    xbreaks <- list(date_labels = "%H:%M", date_breaks = "1 hour")
    if (is.null(x_lab)) x_lab <- "Hour"
  } else if (freq %in% c("daily", "weekly")) {
    scale_func <- scale_x_date
    xbreaks <- list(date_labels = "%m-%d", date_breaks = "1 day")
    if (is.null(x_lab)) x_lab <- "Date (mm-dd)"
  } else if (freq %in% c("monthly", "quarterly")) {
    scale_func <- scale_x_date
    xbreaks <- list(date_labels = "%m-%d", date_breaks = "1 month")
    if (is.null(x_lab)) x_lab <- "Date (mm-dd)"
  } else {  # yearly
    scale_func <- scale_x_date
    xbreaks <- list(date_labels = "%Y", date_breaks = "1 year")
    if (is.null(x_lab)) x_lab <- "Year"
  }

  if (is.null(y_lab)) y_lab <- "Value"

  if (!all(c("yhat_lower", "yhat_upper") %in% names(data_combined))) {
    stop("data must contain 'yhat_lower' and 'yhat_upper'.")
  }

  # Build plot
  p <- ggplot(data_combined, aes(x = ds)) +
    geom_ribbon(aes(ymin = yhat_lower, ymax = yhat_upper),
                fill = "lightblue", alpha = 0.4) +
    geom_line(aes(y = y), color = "black", size = 1) +
    geom_line(aes(y = yhat), color = "blue", linetype = "dashed", size = 1) +
    do.call(scale_func, xbreaks) +
    labs(
      title = paste0("Forecasts from Ensemble (Ensemble = ", hybrid_forecast$emsemble,
                     ", MAPE = ", round(hybrid_forecast$mape * 100, 1), "%)"),
      x = x_lab,
      y = y_lab,
      ...
    ) +
    theme_minimal() +
    theme(
      legend.position = "none",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )

  return(p)
}






# Simulate time series
set.seed(123)
# Create a time sequence (weekly data for 3 years)
dates <- seq.Date(from = as.Date("2020-01-01"), by = "week", length.out = 156)
# Generate seasonal components
weekly_seasonality <- sin(2 * pi * seq_along(dates) / 7) * 5
quarterly_seasonality <- sin(2 * pi * seq_along(dates) / 13) * 10
# Add random noise
noise <- rnorm(length(dates), mean = 0, sd = 3)
# Generate trend with 5 change points
change_points <- round(seq(1, length(dates), length.out = 6))
trend <- numeric(length(dates))
slopes <- c(0.1, -0.05, 0.2, -0.1, 0.15)
for (i in 1:(length(change_points)-1)) {
  start <- change_points[i]
  end <- change_points[i + 1] - 1
  if (i == 1) {
    trend[start:end] <- slopes[i] * seq(0, end - start)
  } else {
    trend[start:end] <- trend[start - 1] + slopes[i] * seq(1, end - start + 1)
  }
}

# Combine all components
values <- 50 + weekly_seasonality + quarterly_seasonality + noise + trend
# Create data frame
df<- data.frame(ds = dates, y = values)


df <- read.csv('./ed.csv')
df <- read.csv('./rph_ed.csv')
df <- read.csv('./ed_all.csv') %>% filter(establishment_code %in% c(101)) %>% group_by(ds)%>%summarise(y=sum(y),.groups="drop")%>%mutate(ds=as.Date(ds))
df <- read.csv('./ed_all.csv') %>% group_by(ds)%>%summarise(y=sum(y),.groups="drop")


library(forecast)
library(prophet)
library(fpp)


data(ausbeer)

df<-data.frame(y=as.numeric(ausbeer))
df$ds<-as.Date(as.Date(ausbeer))

df<-tail(df,1000)

n<-NROW(df)

HORIZON<-12
ts1=df$y[1:(n-HORIZON)]


m=auto.arima(ts1)
h=forecast(m,h=HORIZON)
m2=ets(ts1)
h2=forecast(m2,h=HORIZON)
m3=prophet(df[1:(n-HORIZON),])

h3=predict(m3,df[(n-HORIZON+1):n,])

smape(tail(df$y,HORIZON),h$mean)
smape(tail(df$y,HORIZON),h2$mean)
smape(tail(df$y,HORIZON),h3$yhat)


### final full model


start_time <- as.POSIXct(max(df$ds)) + 3600  # add 1 hour
hourly_seq <- seq(from = start_time, by = "hour", length.out = HORIZON)
df_future <- data.frame(ds=hourly_seq)

## train the model with data
hybrid_model<-hybrid(data=df,seasonal_periods= NULL, n_changepoints=NA,method=NA,max_lag=NA,horizon=HORIZON)
plot(hybrid_model)

newdata<-data.frame(ds=seq(1+as.Date(max(df$ds)), by = "quarter", length.out = HORIZON))
newdata<-data.frame(ds=seq(1+as.Date(max(df$ds)), by = "day", length.out = HORIZON))
hybrid_forecast<-predict_hybrid(hybrid_model,newdata)
plot(hybrid_forecast)

###double check if cross validation correct

df_test<-head(df,NROW(df)-HORIZON)
hybrid_model<-hybrid(df,seasonal_periods = NULL,n_changepoints =NA, method=NA, max_lag = NA, horizon = HORIZON)

df_future <- data.frame(ds=seq(1+as.Date(max(df_test$ds)), by = "day", length.out = HORIZON))
df_future <- predict_hybrid(hybrid_model,df_future)
smape(df_future$yhat,tail(df$y,HORIZON))


